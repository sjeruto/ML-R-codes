rm(list=ls())
dev.off()
#set working directory
getwd() 
setwd("C:/Users/sharon/Documents/MLAA Assignment 3 (1)") 
getwd() 

###Load libraries
library(tm) 
library(SnowballC)

#Load corpus... 
docs <- VCorpus(DirSource("./docs"))

#Check details
#of docs
print(docs)
class(docs)

#Volatile corpus consisting of 30 documents
#examine contents
docs[1]
class(docs[1])
#Volatile corpus consisting of 1 documents
#To access the actual contents we need to use the [[]] notation
class(docs[[1]])
docs[[1]]$meta
docs[[1]]$content

#Preprocessing

#Remove punctuation - replace punctuation marks with " "
docs <- tm_map(docs, removePunctuation)
#Transform to lower case
docs <- tm_map(docs,content_transformer(tolower))
#Strip digits
docs <- tm_map(docs, removeNumbers)
#Remove stopwords from standard stopword list 
docs <- tm_map(docs, removeWords,stopwords("english"))
#Strip whitespace (cosmetic?)
docs <- tm_map(docs, stripWhitespace)
#inspect output
writeLines(as.character(docs[[30]]))

#Stem document
##########################
docs <- tm_map(docs,stemDocument)

#inspect
writeLines(as.character(docs[[30]]))
#end of preprocessing


##convert the corpus document into a tidy format
library(dplyr)
install.packages("tidytext")
library(tidytext)
library(igraph)
library(tidyr)

t_corpus <- docs %>% tidy()
t_corpus

d_corpus <- t_corpus %>% 
  select(id, text)

##Use the`unnest_tokens which tokenizes the text variable into works and creates one row per token.
##Alos converts all characters to lower cases
tidy_df <- t_corpus %>%
  unnest_tokens(word, text)

tidy_df %>%
  select(id, word) %>%
  head(15)

####Tidytext cleaning
#returns a dataframe with a list of stopwords (frequent but little meaningful words).
New_tify_df <- tidy_df %>%
  anti_join(get_stopwords())
#filter(is.na(as.numeric(word))

New_tify_df                   

## Joining, by = "word" 
## Warning in evalq(is.na(as.numeric(word)), <environment>): NAs introduced by
## coercion

New_tify_df %>%
  select(id, word) 
head(10)


####ANALYSIS
########Term frequencies & TF-IDF
#identify term frequencies of the entire corpus.
New_tify_df %>%
  count(word, sort = TRUE) %>%
  head(10)

#Visualise
New_tify_df %>%
  count(word, sort = TRUE) %>%
  head(10) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip()

##
perb <- New_tify_df %>%
  unnest(id, word) %>%
  count(id, word, sort = TRUE) %>%
  ungroup()

#calculate the overall words in the documents

peridc <- perb %>%
  group_by(id) %>%
  summarize(total = sum(n))

book_words <- left_join(perb, peridc)

book_words

##
library(textdata)
library(dplyr)

###To get frequency in the data
frequency <- New_tify_df %>%
  mutate(str_extract (word, "[a-z']+")) %>%
  count(id, word) %>%
  group_by(id) %>%
  mutate(proportion = n / sum(n)) %>%
  select(-n) 

frequency %>%
  arrange(desc(proportion))

###Zipfâ€™s law states that the frequency tht a word appears
#is inversely proportional to its rank.
freq_by_rank <- book_words %>% 
  group_by(id) %>%
  mutate(rank = row_number(), 
         `term frequency` = n/total)

freq_by_rank

#The rank column ranks each word within the frequency table.
#Visualise
library(ggplot2)
freq_by_rank %>%
  ggplot(aes(rank, `term frequency`, colour = id)) +
  geom_line(size = 1.1, alpha = 0.8, show.legend = FALSE) +
  scale_x_log10() +
  scale_y_log10()
###
rank_subset <- freq_by_rank %>%
  filter(rank<500,
         rank>10)

lm(log10(`term frequency`) ~log10(rank), data = rank_subset)
###log10(rank) above, the slow is -0.7837 .
###Plot  
freq_by_rank %>% 
  ggplot(aes(rank, `term frequency`, color = id)) + 
  geom_abline(intercept = -0.62, slope = -1.1, color = "gray50", linetype = 2) +
  geom_line(size = 1.1, alpha = 0.8, show.legend = FALSE) + 
  scale_x_log10() +
  scale_y_log10()

###
install.packages("janeaustenr")
library(janeaustenr)
library(dplyr)
library(tidytext)
book_words <- book_words %>%
           bind_tf_idf(word, id, n)

book_words

## tf and tf-idf are close to zero because they are extremely common words
book_words %>%
  select(-total) %>%
  arrange(desc(tf_idf))


#### Semantic analysis
sentiments
install.packages("textdata")
library(textdata)

tidy_joy <- get_sentiments("nrc") %>%
  filter(sentiment == "joy")

tidy_df %>% 
  inner_join(tidy_joy) %>%
  count(word, sort = TRUE)

tidy_df_sent <- tidy_df %>%
  inner_join(get_sentiments("bing")) %>%
  count(id, sentiment) %>%
  spread(sentiment, n, fill = 0)

#mutate(sentiment = positve - negative)

tidy_df_sent <- tidy_df_sent %>%
  mutate(sentiment = positive - negative)

tidy_df_sent

ggplot(tidy_df_sent, aes(id, sentiment, fill = id)) +
  geom_col(show.legend = FALSE) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

#We can see here that documents 1-9 have negative sentiments and documents 10 onto 41 are primarily positive.
bing_word_counts <- tidy_df %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup()

bing_word_counts %>%
  group_by(sentiment) %>%
  top_n(10) %>%
  ungroup() %>%
  mutate(word = reorder(word,n)) %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(y = "Contribution to sentiment",
       x = NULL) +
  coord_flip()


#### Load data again for topic modelling
rm(list=ls())
#load tm library
library(tm)
#load files into corpus
docs <- VCorpus(DirSource("./docs"))
print(docs)
#inspect a particular document
writeLines(as.character(docs[[30]]))
#Remove punctuation - replace punctuation marks with " "
docs <- tm_map(docs, removePunctuation)
#Transform to lower case
docs <- tm_map(docs,content_transformer(tolower))
#Strip digits
docs <- tm_map(docs, removeNumbers)
#Remove stopwords from standard stopword list 
docs <- tm_map(docs, removeWords, stopwords("english"))
#Strip whitespace (cosmetic?)
docs <- tm_map(docs, stripWhitespace)
#inspect output
writeLines(as.character(docs[[30]]))
#Stem document
docs <- tm_map(docs,stemDocument)
#inspect
writeLines(as.character(docs[[30]]))
#Create document-term matrix
dtm <- DocumentTermMatrix(docs)
dtm

####

#Topic models
library(topicmodels)

#Run LDA using Gibbs Sampling
burnin <- 1000
# and perform 2000 iterations (after burn-in)...
iter <- 2000
#..taking every 500th one for further use. This "thinning" is done to ensure that
# samples are not correlated.
thin <- 500
#We'll use 5 different, randomly chosen starting points
nstart <- 5
#using random integers as seed. Feel free to change these
seed <- list(2003,5,63,100001,765)
#...and take the best run (the one with the highest probability) as the result
best <- TRUE

#Number of topics (6)
k <- 6

ldaOut <- LDA(dtm,k, method="Gibbs", control=
                list(nstart=nstart, seed = seed, best=best, burnin = burnin, iter = iter, thin=thin))
topics(ldaOut)
ldaOut.topics <-as.matrix(topics(ldaOut))

###Number of topics (20)
#Number of topics (6)
k <- 20

ldaOut <- LDA(dtm,k, method="Gibbs", control=
                list(nstart=nstart, seed = seed, best=best, burnin = burnin, iter = iter, thin=thin))
topics(ldaOut)
ldaOut.topics <-as.matrix(topics(ldaOut))
write.csv(ldaOut.topics,file=paste("LDAGibbs",k,"DocsToTopics.csv"))
terms(ldaOut,12)
ldaOut.terms <- as.matrix(terms(ldaOut,12))
write.csv(ldaOut.terms,file=paste("LDAGibbs",k,"TopicsToTerms.csv"))

###


#Find probabilities associated with each topic assignment
topicProbabilities <- as.data.frame(ldaOut@gamma) 
write.csv(topicProbabilities,file=paste("LDAGibbs",k,"TopicProbabilities.csv"))


# To get posterior distributions
tmResult <- posterior(ldaOut)

# visualize the terms as wordcloud
top5termsPerTopic <- terms(ldaOut, 5)
topicNames <- apply(top5termsPerTopic, 2, paste, collapse=" ")
# visualize topics as word cloud
topicToViz <- 18 # change for your own topic of interest
topicToViz <- grep('risk', topicNames)[1] # Or select a topic by a term contained in its name
# select to 40 most probable terms from the topic by sorting the term-topic-probability vector in decreasing order
top40terms <- sort(tmResult$terms[topicToViz,], decreasing=TRUE)[1:40]
words <- names(top40terms)
# extract the probabilites of each of the 40 terms
probabilities <- sort(tmResult$terms[topicToViz,], decreasing=TRUE)[1:40]
# visualize the terms as wordcloud
mycolors <- brewer.pal(8, "Dark2")
wordcloud(words, probabilities, random.order = FALSE, color = mycolors)


