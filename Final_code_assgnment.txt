####CODE for ngrams and topic modelling;


rm(list=ls())
dev.off()
#set working directory
getwd() 
setwd("C:/Users/sharon/Documents/MLAA Assignment 3 (1)") 
getwd() 

###Load libraries
library(tm) 
library(SnowballC)

#Load corpus... 
docs <- VCorpus(DirSource("./docs"))

#Check details
#of docs
print(docs)
class(docs)

#Volatile corpus consisting of 30 documents
#examine contents
docs[1]
class(docs[1])
#Volatile corpus consisting of 1 documents
#To access the actual contents we need to use the [[]] notation
class(docs[[1]])
docs[[1]]$meta
docs[[1]]$content

#Preprocessing

#Remove punctuation - replace punctuation marks with " "
docs <- tm_map(docs, removePunctuation)
#Transform to lower case
docs <- tm_map(docs,content_transformer(tolower))
#Strip digits
docs <- tm_map(docs, removeNumbers)
#Remove stopwords from standard stopword list 
docs <- tm_map(docs, removeWords,stopwords("english"))
#Strip whitespace (cosmetic?)
docs <- tm_map(docs, stripWhitespace)
#inspect output
writeLines(as.character(docs[[30]]))

#Stem document
##########################
docs <- tm_map(docs,stemDocument)

#inspect
writeLines(as.character(docs[[30]]))
#end of preprocessing

#Create document-term matrix
dtm <- DocumentTermMatrix(docs)
#summary
dtm
#inspect segment of document term matrix
inspect(dtm[1:10,1000:1006])
#collapse matrix by summing over columns - this gets total counts (over all docs) for each term
freq <- colSums(as.matrix(dtm))
#length should be total number of terms
length(freq)
#create sort order (asc)
ord <- order(freq,decreasing=TRUE)
#inspect most frequently occurring terms
freq[head(ord)]
#write to disk and inspect file
write.csv(file="freq.csv",freq[ord])
#inspect least frequently occurring terms
freq[tail(ord)]
#list most frequent terms. Lower bound specified as second argument
findFreqTerms(dtm,lowfreq=80)
#correlations
findAssocs(dtm,"project",0.95)
findAssocs(dtm,"risk manag ",0.75)
findAssocs(dtm,"mont carlo",0.9)
findAssocs(dtm,"time",0.9)
findAssocs(dtm,"topic",0.9)
findAssocs(dtm,"probabl",0.9)
findAssocs(dtm,"techniqu",0.9)
findAssocs(dtm,"validity",0.9)

#histogram
library(ggplot2)
wf=data.frame(term=names(freq),occurrences=freq)
p <- ggplot(subset(wf, occurrences>200), aes(term, occurrences))
p <- p + geom_bar(stat="identity")
p <- p + theme(axis.text.x=element_text(angle=45, hjust=1))
p
#order by frequency
p <- ggplot(subset(wf, occurrences>200), aes(reorder(term,occurrences), occurrences))
p <- p + geom_bar(stat="identity")
p <- p + theme(axis.text.x=element_text(angle=45, hjust=1))
p
#wordcloud
library(wordcloud)
#setting the same seed each time ensures consistent look across clouds
set.seed(42)
#limit words by specifying min frequency
wordcloud(names(freq),freq, max.words=100)
#...add color
wordcloud(names(freq),freq,max.words=100,colors=brewer.pal(6,"Dark2"))

#try specifying min.freq instead of max.words

##n-grams
#######################

##create bigram tokenizer
#uses ngrams function from NLP package

##ngrams
#to see what ngrams does, try running ngrams(words(docs[[1]]$content),2), which
#returns bigrams for the first document in the corpus
ngrams(words(docs[[1]]$content),2)
##unlist creates a vector from the atomic components of a list

BigramTokenizer <-  function(x) unlist(lapply(ngrams(words(x), 2), paste, collapse = " "), use.names = FALSE)
#create DTM
dtmbi <- DocumentTermMatrix(docs, control = list(tokenize = BigramTokenizer))
freqbi <- colSums(as.matrix(dtmbi))
#length should be total number of terms
length(freqbi)
#create sort order (asc)
ordbi <- order(freqbi,decreasing=TRUE)
#inspect most frequently occurring terms
freqbi[head(ordbi)]
##plot bigram
wordcloud(names(freqbi),freqbi, max.words=100)
#order bigram by frequency
wf_bi=data.frame(term=names(freqbi),occurrences=freqbi)
p_bi <- ggplot(subset(wf_bi, occurrences>100), aes(reorder(term,occurrences), occurrences))
p_bi <- p_bi + geom_bar(stat="identity")
p_bi <- p_bi + theme(axis.text.x=element_text(angle=45, hjust=1))
p_bi

##Trigram;
#returns trigrams for the first document in the corpus
ngrams(words(docs[[1]]$content),3)
##unlist creates a vector from the atomic components of a list

TrigramTokenizer <-  function(x) unlist(lapply(ngrams(words(x), 3), paste, collapse = " "), use.names = FALSE)
#create DTM
dtmbi_tri <- DocumentTermMatrix(docs, control = list(tokenize = TrigramTokenizer))
freqtri <- colSums(as.matrix(dtmbi_tri))
#length should be total number of terms
length(freqtri)
#create sort order (asc)
ordtri <- order(freqtri,decreasing=TRUE)
#inspect most frequently occurring terms
freqtri[head(ordtri)]
##plot trigram
wordcloud(names(freqtri),freqtri, max.words=50)
#order trigram by frequency
wf_tri=data.frame(term=names(freqtri),occurrences=freqtri)
p_tri <- ggplot(subset(wf_tri, occurrences>17), aes(reorder(term,occurrences), occurrences))
p_tri <- p_tri + geom_bar(stat="identity")
p_tri <- p_tri + theme(axis.text.x=element_text(angle=45, hjust=1))
p_tri

##Quadgram;
#returns trigrams for the first document in the corpus
ngrams(words(docs[[1]]$content),4)
##unlist creates a vector from the atomic components of a list

QuadgramTokenizer <-  function(x) unlist(lapply(ngrams(words(x), 4), paste, collapse = " "), use.names = FALSE)
#create DTM
dtmbi_quad <- DocumentTermMatrix(docs, control = list(tokenize = QuadgramTokenizer))
freqQuad <- colSums(as.matrix(dtmbi_quad))
#length should be total number of terms
length(freqQuad)
#create sort order (asc)
ordQuad <- order(freqQuad,decreasing=TRUE)
#inspect most frequently occurring terms
freqQuad[head(ordQuad)]
##plot trigram
wordcloud(names(freqQuad),freqQuad, max.words=50)
#order trigram by frequency
wf_quad=data.frame(term=names(freqQuad),occurrences=freqQuad)
p_quad <- ggplot(subset(wf_quad, occurrences>9), aes(reorder(term,occurrences), occurrences))
p_quad <- p_quad + geom_bar(stat="identity")
p_quad <- p_quad + theme(axis.text.x=element_text(angle=45, hjust=1))
p_quad


####Topic modelling
rm(list=ls())
dev.off()
#set working directory
getwd() 
setwd("C:/Users/sharon/Documents/MLAA Assignment 3 (1)") 
getwd() 
#load tm library
library(tm)
#load files into corpus
docs <- VCorpus(DirSource("./docs"))
print(docs)
#inspect a particular document
writeLines(as.character(docs[[30]]))
#Remove punctuation - replace punctuation marks with " "
docs <- tm_map(docs, removePunctuation)
#Transform to lower case
docs <- tm_map(docs,content_transformer(tolower))
#Strip digits
docs <- tm_map(docs, removeNumbers)
#Remove stopwords from standard stopword list 
docs <- tm_map(docs, removeWords, stopwords("english"))
#Strip whitespace (cosmetic?)
docs <- tm_map(docs, stripWhitespace)
#inspect output
writeLines(as.character(docs[[30]]))
#Stem document
docs <- tm_map(docs,stemDocument)
#inspect
writeLines(as.character(docs[[30]]))
#Create document-term matrix
dtm <- DocumentTermMatrix(docs)
dtm

####

########Topic models#################
library(topicmodels)

#Run LDA using Gibbs Sampling
burnin <- 1000
# and perform 2000 iterations (after burn-in)...
iter <- 2000
#..taking every 500th one for further use. This "thinning" is done to ensure that
# samples are not correlated.
thin <- 500
#We'll use 5 different, randomly chosen starting points
nstart <- 5
#using random integers as seed. Feel free to change these
seed <- list(2003,5,63,100001,765)
#...and take the best run (the one with the highest probability) as the result
best <- TRUE

#Number of topics (6)
k <- 6

ldaOut1 <- LDA(dtm,k, method="Gibbs", control=
                list(nstart=nstart, seed = seed, best=best, burnin = burnin, iter = iter, thin=thin))
topics(ldaOut1)
ldaOut.topics1 <-as.matrix(topics(ldaOut))

##compare
###Number of topics (20)
k <- 20

ldaOut <- LDA(dtm,k, method="Gibbs", control=
                list(nstart=nstart, seed = seed, best=best, burnin = burnin, iter = iter, thin=thin))
topics(ldaOut)
ldaOut.topics <-as.matrix(topics(ldaOut))
write.csv(ldaOut.topics,file=paste("LDAGibbs",k,"DocsToTopics.csv"))
terms(ldaOut,12)
ldaOut.terms <- as.matrix(terms(ldaOut,12))
write.csv(ldaOut.terms,file=paste("LDAGibbs",k,"TopicsToTerms.csv"))

###


#Find probabilities associated with each topic assignment
topicProbabilities <- as.data.frame(ldaOut@gamma) 
write.csv(topicProbabilities,file=paste("LDAGibbs",k,"TopicProbabilities.csv"))


# To get posterior distributions
tmResult <- posterior(ldaOut)

# for every document we have a probaility distribution of its contained topics
theta <- tmResult$topics 
dim(theta)  

# visualize the terms as wordcloud
library(wordcloud)
top5termsPerTopic <- terms(ldaOut, 5)
topicNames <- apply(top5termsPerTopic, 2, paste, collapse=" ")
# visualize topics as word cloud
topicToViz <- 18 # change for your own topic of interest
topicToViz <- grep('risk', topicNames)[1] # Or select a topic by a term contained in its name
# select to 40 most probable terms from the topic by sorting the term-topic-probability vector in decreasing order
top40terms <- sort(tmResult$terms[topicToViz,], decreasing=TRUE)[1:40]
words <- names(top40terms)
# extract the probabilites of each of the 40 terms
probabilities <- sort(tmResult$terms[topicToViz,], decreasing=TRUE)[1:40]
# visualize the terms as wordcloud
mycolors <- brewer.pal(8, "Dark2")
wordcloud(words, probabilities, random.order = FALSE, color = mycolors)

##Visualise the distribution in the documents
exampleIds <- c(2, 25, 42)
lapply(docs[exampleIds], as.character)

install.packages("pals")
library(pals)
library(reshape2)
library(ggplot2)

###visualize the topic distributions within the documents.
N <- length(exampleIds)
# get topic proportions form example documents
topicProportionExamples <- theta[exampleIds,]
colnames(topicProportionExamples) <- topicNames
vizDataFrame <- melt(cbind(data.frame(topicProportionExamples), document = factor(1:N)), variable.name = "topic", id.vars = "document")  
ggplot(data = vizDataFrame, aes(topic, value, fill = document), ylab = "proportion") + 
  geom_bar(stat="identity") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +  
  coord_flip() +
  facet_wrap(~ document, ncol = N)

###Topic ranking
#re-rank top topic terms for topic names
topicNames <- apply(lda::top.topic.words(beta, 5, by.score = T), 2, paste, collapse = " ")

#We sort topics according to their probability within the entire collection:
# What are the most probable topics in the entire collection?
topicProportions <- colSums(theta) / nDocs(dtm)  # mean probablities over all paragraphs
names(topicProportions) <- topicNames     # assign the topic names we created before
sort(topicProportions, decreasing = TRUE) # show summed proportions in decreased order
##
soP <- sort(topicProportions, decreasing = TRUE)
paste(round(soP, 5), ":", names(soP))




















